{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIS 545"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d271f53de90b0dc50f0ca9072ab2a34d",
     "grade": false,
     "grade_id": "step5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# CIS 545 Homework 3: Advanced Portion\n",
    "\n",
    "## Step 5: tf-idf search engine\n",
    "Now that you have implemented PageRank, which tells you how authoritative websites are, you are going to build the other very important part of a search engine. Namely, this part will retrieve the documents that are _relevant_ to a given query. Simply put, this part will give you a subset of webpages that are presumed to be relevant to the query and then PageRank can rerank them according to how much you value authoritativeness over relevance. The methods in this section are a basis of both _keyword search_, and also _document clustering_.  (We’ll see a lot more about clustering in a few weeks.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "efeee5289cead6030f3f5d94e92cfd61",
     "grade": false,
     "grade_id": "step5-1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Step 5.1: Preliminaries\n",
    "\n",
    "### Step 5.1.1 Load nltk and some helpful tools\n",
    "\n",
    "The cell below gives you some tools and assigns some variables that you will need later. You do not need to modify this cell provided that you have everything installed already."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cbf281415bceb27775f9fa0334757e29",
     "grade": false,
     "grade_id": "hw3-import_nltk",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "98c464dd11c2101992ad0c81a8fb52e7",
     "grade": false,
     "grade_id": "hw3-stopfilesetup",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [hw3-stopfilesetup]\n",
    "# This imports your data and sets up nltk parsers\n",
    "# You should not have to modify the cell\n",
    "\n",
    "\"\"\"\n",
    "# Returns True if the input (string) parameter has\n",
    "# any sort of letter in it, else returns False.\n",
    "\"\"\"\n",
    "def has_letter(x):\n",
    "    return re.match('.*[a-zA-Z].*',x) != None\n",
    "\n",
    "# Stopwords are words we will ignore for search\n",
    "# purposes, because they are too common to be useful\n",
    "stopwords = set()\n",
    "\n",
    "# Import the data - the S3 link is only for grading, and you should still use the stopwords.txt for now\n",
    "try:\n",
    "    stop_file = open('stopwords.txt')\n",
    "except:\n",
    "    stop_file = pd.read_csv('https://s3.amazonaws.com/cis545-hw3data/stopwords.txt', header = None)[0].tolist()\n",
    "\n",
    "for line in stop_file:\n",
    "    stopwords.add(line.strip())\n",
    "\n",
    "# The NLTK parser breaks apostrophe-s into a separate \"word\"\n",
    "# so we'll want to add it to the list... Though it's technically\n",
    "# not a stop word in the traditional sense.\n",
    "stopwords.add(\"'s\")\n",
    "\n",
    "# Create the word stemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5b6a792494921e350a61a1d064294549",
     "grade": false,
     "grade_id": "cis545-pad-cell1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "47359cd1f7c7d051cd87f1c41628533d",
     "grade": false,
     "grade_id": "step5-1-2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 5.1.2. Import text files, clean them, and store in a dictionary \n",
    "\n",
    "As a \"corpus\" we fetched some data from Wikipedia, based on currently\n",
    "trendy topics.  Each topic had multiple interpretations, some of which \n",
    "we suspected would \"intersect\" in interesting ways (e.g., Trump/Putin, Cloud/Google, \n",
    "Cloud/Climate).  Others had various interpretations (e.g., there are many types of \n",
    "Football).  See _Wikipedia.ipynb_ for the original download code. - these are from **text.zip**\n",
    "\n",
    "Selected topics (for which the top-10 matches were returned by Wikipedia) were:\n",
    "\n",
    " * Pennsylvania\n",
    " * Trump\n",
    " * Apple\n",
    " * Google\n",
    " * Farm\n",
    " * Climate\n",
    " * Cloud\n",
    " * Football\n",
    " * Government\n",
    " * Putin\n",
    " \n",
    "Please write the function `clean_article(article)` that takes a string as input and then:\n",
    "\n",
    "1. tokenizes it using `nltk.word_tokenize` (this one is not for Twitter)\n",
    "2. converts it to lower case\n",
    "3. removes stopwords that are present in the set from the cell above\n",
    "4. uses `stemmer` (defined above) to cut the word down to its stem\n",
    "5. uses `has_letter` (defined above) to remove words that don't have any letters\n",
    "6. keeps only those words with length greater than 1 and less than 20.\n",
    "\n",
    "It is important that you perform these steps in order so your result will be the same as ours!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"'s\" in stopwords"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import re\n",
    "has_letter('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bafac1c1fd4fc34117e12e49e0d16b66",
     "grade": false,
     "grade_id": "hw3-clean_article",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [hw3-clean_article]\n",
    "# TODO: Write the clean_article function, noting that 'article' is a string input\n",
    "# for i in range(0,len(clean_incidents)):\n",
    "#     word_raw = nltk.word_tokenize(clean_incidents[i])#nltk 停用词问题\n",
    "#     word = [w for w in word_raw if(w not in stopwords.words('english'))]\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "\n",
    "#words = stopwords.words('english')\n",
    "\n",
    "def clean_article(article):\n",
    "    tokens = nltk.word_tokenize(article)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [token for token in tokens if(token not in stopwords)]\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    tokens = [token for token in tokens if(has_letter(token)==True)]\n",
    "    tokens = [token for token in tokens if(len(token)>1 and len(token)<20)]\n",
    "    return tokens\n",
    "    #print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c0e5962c6c6423add60ac95c34a9aeab",
     "grade": false,
     "grade_id": "hw3-loadfiles",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files loaded\n"
     ]
    }
   ],
   "source": [
    "# [hw3-loadfiles]\n",
    "# Run: Load files for test cases \n",
    "# You should unzip test.zip prior to running this\n",
    "\n",
    "docs = {}\n",
    "\n",
    "try:\n",
    "    # Local directory - to be used during homework\n",
    "    for filename in os.listdir('text'):\n",
    "        # Opens files and cleans to import\n",
    "        file = open('text/' + filename)\n",
    "        docs[filename] = clean_article(file.read())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    # Used for grading only - the link will not be active until you finish submitting!\n",
    "    # Pulls list of files from AWS\n",
    "    r = requests.get('https://s3.amazonaws.com/cis545-hw3data/list_of_texts.txt')\n",
    "    r.encoding = 'utf-8'\n",
    "    filenames = r.text.split('\\n')\n",
    "    for textfile in filenames:\n",
    "        \n",
    "        # Determines filenames\n",
    "        filename = textfile.strip()\n",
    "        filepath = filename.replace('+', '%2B')\n",
    "        filepath = filepath.replace(',', '%2C')\n",
    "        filepath = filepath.replace(' ', '+')\n",
    "        \n",
    "        # Extracts our data\n",
    "        filepath = 'https://s3.amazonaws.com/cis545-hw3data/text/' + filepath\n",
    "        r = requests.get(filepath)\n",
    "        r.encoding = 'utf-8'        \n",
    "        \n",
    "        # Cleans UTF-8 data\n",
    "        docs[filename] = clean_article(r.text)\n",
    "\n",
    "print (\"All files loaded\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cd73c675ea8430ec4108f4b8319b5907",
     "grade": true,
     "grade_id": "cis545-pad-cell2",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Homework 3] Padding\n"
     ]
    }
   ],
   "source": [
    "# [hw3-pad]\n",
    "# This cell is for grading purposes only! Please DO NOT modify it.\n",
    "\n",
    "\n",
    "print(\"[CIS 545 Homework 3] Padding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "307492ef094be63ac0b6cf7fecea9629",
     "grade": true,
     "grade_id": "test-docs",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [test-docs]\n",
    "# [CIS 545 Homework 3] Test Case\n",
    "# Testing your values in docs - that things were loaded correctly\n",
    "\n",
    "assert(docs['Apple.txt'][20] == 'central')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "affb3111090e31597dda09ae51d74ea3",
     "grade": false,
     "grade_id": "test-pad30",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Homework 3] Test Case - 2 points\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d17916355b36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# [CIS 545 Test Cases] (2 pts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[CIS 545 Homework 3] Test Case - 2 points\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'score' is not defined"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (2 pts)\n",
    "print(\"[CIS 545 Homework 3] Test Case - 2 points\")\n",
    "score = score + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2374a6db9bdfbf22eacc6d373b03dc1a",
     "grade": false,
     "grade_id": "step5-2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Step 5.2. Generate the Vocabulary\n",
    "\n",
    "As discussed in class, natural language words follow a **Zipfian distribution**, which means that many, sometimes over half, of the unqiue words in a collection of documents only occur once. Limiting the **vocabulary**, or the list of words that are considered for a data science task, often improves both the efficiency and accuracy of the system. Now, we are going to limit the vocabulary of our search engine using a minimum number of occurrences. In the next cell, you should:\n",
    "\n",
    "1. Create a single list containing the words of each (cleaned) document in succession.\n",
    "2. Count occurrences of each word. You may use `nltk.FreqDist` if you wish but you do not have to.\n",
    "3. Remove all words that only occurred once. You may use `hapaxes` if you wish but you do not have to.\n",
    "4. Make a list called `wordids` that contains the words in decreasing frequency order. For example, the most frequent word in this document collection is \"appl\", so `wordids[0] = 'appl'`.\n",
    "5. Make a dictionary called `lexicon` that has words as keys and their indices in `wordids` as values."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for value in docs:\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "word_freq[\"—an\"]\n",
    "#print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "268cd668e3c76c819294b052503ab5e3",
     "grade": false,
     "grade_id": "hw3-create-lexicon",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [hw3-create-lexicon]\n",
    "# TODO: Create your vocabulary here. \n",
    "# We will test the list named wordids and the dictionary named lexicon\n",
    "from collections import Counter\n",
    "word_list = []\n",
    "for key, value in docs.items():\n",
    "    #print(value)\n",
    "    word_list.extend(value)\n",
    "#print(word_list)\n",
    "word_freq = nltk.FreqDist(word_list).most_common()\n",
    "#print(word_freq)\n",
    "wordids = [key for key,value in word_freq if(value !=1)]\n",
    "#print(wordids)\n",
    "lexicon = {}\n",
    "i=0\n",
    "for item in wordids:\n",
    "    lexicon[item]=i\n",
    "    i=i+1\n",
    "#print(lexicon)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lexicon['appl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5460df2e23d68d55949e054d63f4defe",
     "grade": true,
     "grade_id": "test-lexicon1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11074 words in your lexicon.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['googl',\n",
       " 'trump',\n",
       " 'state',\n",
       " 'appl',\n",
       " 'cloud',\n",
       " 'use',\n",
       " 'also',\n",
       " 'footbal',\n",
       " 'first',\n",
       " 'new']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [test-lexicon1]\n",
    "# [CIS 545 Homework 3] Test Case (test case pad below!)\n",
    "# Testing your wordids (public test only)\n",
    "\n",
    "print(\"There are\", len(lexicon), \"words in your lexicon.\")\n",
    "display(wordids[:10])\n",
    "assert(wordids[3] == 'appl')\n",
    "assert(wordids[7] == 'footbal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "043206cd30840c9767e60ec864cdbce6",
     "grade": false,
     "grade_id": "test-pad31",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Homework 3] Test Case - 1 points\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2380b639b20a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# [CIS 545 Test Cases] (1 pts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[CIS 545 Homework 3] Test Case - 1 points\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'score' is not defined"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (1 pts)\n",
    "print(\"[CIS 545 Homework 3] Test Case - 1 points\")\n",
    "score = score + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5edb38f8fc3236e8985a5359122d405c",
     "grade": true,
     "grade_id": "test-lexicon2",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [test-lexicon2]\n",
    "# [CIS 545 Homework 3] Test Case (test case pad below!)\n",
    "# Testing your wordids (hidden tests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6841472425ba896937f8695b9d54dab2",
     "grade": false,
     "grade_id": "test-pad32",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Homework 3] Test Case - 2 points\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d17916355b36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# [CIS 545 Test Cases] (2 pts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[CIS 545 Homework 3] Test Case - 2 points\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'score' is not defined"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (2 pts)\n",
    "print(\"[CIS 545 Homework 3] Test Case - 2 points\")\n",
    "score = score + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "651056ebc0c7055d2207e2557ed8aeab",
     "grade": false,
     "grade_id": "step5-3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Step 5.3. Term Frequencies\n",
    "\n",
    "We are now going to create a document-term matrix. Write a function `doc_vector(content, lexicon)` that given a cleaned article (as a list of words) and the lexicon, creates a vector of term frequencies. We will define **term frequency** as the number of times the word occurs in the document divided by the number of times the most frequent word in the document occurs. For example, in the document `['a', 'b', 'b']`, 'a' has a term frequency of 0.5 and 'b' has a term frequency of 1.0.\n",
    "\n",
    "For greatest portability, please use the length of the vocabulary (`lexicon`) to determine the length of the output vector."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bafd7e990174d9dcf5e9d7dca11a5a0c",
     "grade": false,
     "grade_id": "hw3-docvector",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "# [hw3-docvector]\n",
    "# TODO:Write your doc_vector function here\n",
    "\n",
    "def doc_vector(content, lexicon):\n",
    "    term_frequency = {}\n",
    "    for key in lexicon:\n",
    "        term_frequency[key]=0\n",
    "    word_list = nltk.FreqDist(content).most_common()\n",
    "    largest = word_list[0][1]\n",
    "    for key,value in word_list:\n",
    "        if key in term_frequency:\n",
    "            term_frequency[key]=value/largest\n",
    "    print(term_frequency)\n",
    "    return term_frequency\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_vector(content, lexicon):\n",
    "    term_frequency = [0]*len(lexicon)\n",
    "    #print(term_frequency)\n",
    "    word_list = nltk.FreqDist(content).most_common()\n",
    "    largest = word_list[0][1]\n",
    "    for key,value in word_list:\n",
    "        if key in lexicon:\n",
    "            term_frequency[lexicon[key]]=value/largest\n",
    "#     print(term_frequency)\n",
    "    return term_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "97870e736b42e5e97821587641f844a2",
     "grade": false,
     "grade_id": "note-testdoc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The next cell is going to assemble your document-term matrix. It is also going to test whether your `doc_vector` function can handle the case in which you provide an \"article\" with no words in the vocabulary. This test is visible to you, so if you pass it, you are fine. You should not need to edit it.\n",
    "\n",
    "The hidden test is a more standard use case in which the article contains some valid word or words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ab3b2258e3f6965fcb3b6ec392644ee1",
     "grade": false,
     "grade_id": "hw3-assemblevector",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [hw3-assemblevector]\n",
    "# Assembles document-term matrix\n",
    "\n",
    "vectors = []\n",
    "doclist = []\n",
    "\n",
    "for topic in docs:\n",
    "    doclist.append(topic)\n",
    "    vectors.append(doc_vector(docs[topic], lexicon))\n",
    "\n",
    "vectors = np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11074\n"
     ]
    }
   ],
   "source": [
    "print(len(lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doclist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_article('to be or not to Be')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b392db275e373d554b94b1fcc3927ce8",
     "grade": true,
     "grade_id": "test-hamlet",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-97e1ec81e82c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Tests against an example with some valid words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'to be or not to Be'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-ff8567dfe848>\u001b[0m in \u001b[0;36mdoc_vector\u001b[0;34m(content, lexicon)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#print(term_frequency)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mword_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mlargest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# [test-hamlet]\n",
    "# Tests against an example with some valid words\n",
    "\n",
    "assert(sum(doc_vector(clean_article('to be or not to Be'), lexicon)) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cd2bcb28c31fc28d6775b3ec9f93b51d",
     "grade": false,
     "grade_id": "test-pad33",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [CIS 545 Test Cases] (1 pt)\n",
    "print(\"[CIS 545 Homework 3] Test Case - 1 point\")\n",
    "score = score + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "84d72f145d453269fe7978996ade9129",
     "grade": true,
     "grade_id": "test-vector1",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [test-vector1]\n",
    "# Tests against a text example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b697381548e354d180445320f039b06a",
     "grade": false,
     "grade_id": "test-pad34",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [CIS 545 Test Cases] (2 pts)\n",
    "print(\"[CIS 545 Homework 3] Test Case - 2 points\")\n",
    "score = score + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "545d3041ec5610929a5ffd698bc20bb2",
     "grade": false,
     "grade_id": "note-listdisplay",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The next cell is going to display the counts of all words in the Putinism document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fb30413f3da1ee5adc48cca98851b277",
     "grade": false,
     "grade_id": "hw3-createlistwords",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state x 0.7916666666666666',\n",
       " 'use x 0.08333333333333333',\n",
       " 'also x 0.25',\n",
       " 'first x 0.041666666666666664',\n",
       " 'new x 0.16666666666666666',\n",
       " 'includ x 0.041666666666666664',\n",
       " 'can x 0.08333333333333333',\n",
       " 'putin x 1.0',\n",
       " 'one x 0.16666666666666666',\n",
       " 'year x 0.041666666666666664',\n",
       " 'govern x 0.20833333333333334',\n",
       " 'unit x 0.08333333333333333',\n",
       " 'compani x 0.041666666666666664',\n",
       " 'servic x 0.20833333333333334',\n",
       " 'may x 0.16666666666666666']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [hw3-createlistwords]\n",
    "# Creates the counts of words\n",
    "# This should be the Putinism document\n",
    "\n",
    "putin_docid = doclist.index(\"Putinism.txt\")\n",
    "list_of_words = []\n",
    "for ident in range(0, len(lexicon)):\n",
    "    if vectors[putin_docid, ident] > 0:\n",
    "        list_of_words.append(wordids[ident] + ' x ' + str(vectors[putin_docid, ident]))\n",
    "        \n",
    "display(list_of_words[:15])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "abs(vectors[12][16] * 6000 - 99.44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0b8082e723c718547a4981a0bcb0083d",
     "grade": true,
     "grade_id": "test-listofwords1",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [test-listofwords1]\n",
    "# Test cases for values in vectors and list of words\n",
    "\n",
    "### BEGI1 HIDDEN TESTS\n",
    "ep = 0.02 # Error amount\n",
    "assert(abs(vectors[2][2]*14 - 3) < ep) # Should be 0\n",
    "assert(abs(vectors[12][16] * 6000 - 99.44) < ep/2) # Should be ~0.0075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a8b40e8a14e6b592ed00c6b2d0de2b49",
     "grade": false,
     "grade_id": "test-pad35",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [CIS 545 Test Cases] (2 pts)\n",
    "print(\"[CIS 545 Homework 3] Test Case - 2 points\")\n",
    "score = score + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "219e64c580640c080e1fb4f939e5349c",
     "grade": true,
     "grade_id": "test-listofwords2",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [test-listofwords2]\n",
    "# Test cases for values in vectors and list of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a9f9910c7b084cc47006b5b3a1d99406",
     "grade": false,
     "grade_id": "test-pad36",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [CIS 545 Test Cases] (2 pts)\n",
    "print(\"[CIS 545 Homework 3] Test Case - 2 points\")\n",
    "score = score + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bbe30a7d4d439374e26f84f75cb23186",
     "grade": false,
     "grade_id": "step5-4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Step 5.4. Inverse Document Frequencies\n",
    "\n",
    "We would like to give **rare words** a higher weight than ones found in all documents.  To measure a word’s rareness, we will develop a metric called **inverse document frequency (idf)**.  In its simplest form, a word’s idf is a ratio between the total number of documents, and how many documents include a word.  (Note that the idf is independent of a given document -- it is a measure of the word’s popularity across the full set of documents, sometimes called a **corpus**.)  Typically, we don’t directly use the ratio, however -- instead we use its base-10 logarithm.  Thus, we define:\n",
    "\n",
    "$$idf(w) = \\log(\\frac{\\text{total number of docs}}{\\text{number of docs containing }w})$$\n",
    "\n",
    "Now compute a single vector `idf` representing, for each word, its idf within the corpus."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2f7f208eb678de13a053f407984bdd49",
     "grade": false,
     "grade_id": "hw3-idf",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [hw3-idf]\n",
    "# TODO: Compute inverse document frequencies\n",
    "import math\n",
    "\n",
    "def func_idf(total_num, x):\n",
    "    count=0\n",
    "    for key, value in docs.items():\n",
    "        if x in value:\n",
    "            count=count+1\n",
    "    idf = math.log10(total_num/count)\n",
    "    return idf\n",
    "\n",
    "\n",
    "total_num = len(docs)\n",
    "#print(total_num)\n",
    "idf = [func_idf(total_num, w) for w in wordids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c7f92902061a3f41c5c8a27e34253725",
     "grade": true,
     "grade_id": "test-idf1",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "googl 0.7269987279362623\n",
      "trump 0.7518223116612945\n",
      "state 0.06319314066349449\n",
      "appl 0.5843312243675307\n",
      "cloud 0.5843312243675307\n",
      "use 0.06845738065585169\n",
      "also 0.023229840718474823\n",
      "footbal 0.5509074688805811\n",
      "first 0.06845738065585169\n",
      "new 0.05799194697768673\n",
      "climat 0.5351132016973491\n",
      "includ 0.05285230732527567\n",
      "can 0.11303951330859223\n",
      "citi 0.20411998265592482\n",
      "putin 0.8683278807327317\n",
      "one 0.07378621416091864\n",
      "game 0.38021124171160603\n",
      "year 0.07378621416091864\n",
      "govern 0.24987747321659987\n",
      "unit 0.10145764075877703\n"
     ]
    }
   ],
   "source": [
    "# [test-idf1]\n",
    "# Test cases for values in IDF\n",
    "\n",
    "# Print first 20 to view \n",
    "for i in range(0, 20):\n",
    "    print(wordids[i], idf[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a1252374f3f3a219bef314de4cf457c9",
     "grade": false,
     "grade_id": "test-pad37",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [CIS 545 Test Cases] (2 pts)\n",
    "print(\"[CIS 545 Homework 3] Test Case - 2 points\")\n",
    "score = score + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3e1c0e9502842ef46a6cee2614212790",
     "grade": true,
     "grade_id": "test-idf2",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [test-idf2]\n",
    "# Test cases for values in IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "114900626f710599ee0e8a8c36f89714",
     "grade": false,
     "grade_id": "test-pad38",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [CIS 545 Test Cases] (2 pts)\n",
    "print(\"[CIS 545 Homework 3] Test Case - 2 points\")\n",
    "score = score + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "18406e8d6dd21ad129306696858476d7",
     "grade": false,
     "grade_id": "step5-5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Step 5.5. Produce Ranked Results\n",
    "\n",
    "As we described previously, standard search simply takes a keyword query and treats it as another document.  This how you would generate a query vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e4a53945408be460f239fafd8cebe2c6",
     "grade": false,
     "grade_id": "hw3-genquery",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3768700ebd6345c2a2dbf1fde8ea5aa9",
     "grade": false,
     "grade_id": "note-cosinesim",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "To compute the similarity between two documents, we use the cosine of the angle between their vectors, given by:\n",
    "\n",
    "$$sim(d_1,d_2) = \\frac{d_1 \\cdot d_2}{||d_1||\\;||d_2||}$$\n",
    "\n",
    "where $||d_i||$ is the Frobenius ($L_2$) norm of the document vector. Write a function `search(vectors, doclist, idf, query, num_results)` that, when given a 2D array of document vectors, a list of document names, idf scores for each word, a query vector, and the desired number of results:\n",
    "\n",
    "1. Creates a Pandas DataFrame with schema (docid, docname, score) for the results of matching the query against each document.\n",
    "2. Scales the words in the document and query vectors by their idfs.\n",
    "3. Computes the cosine similarity score for the query against each document. Hint: Uses Numpy multiplication (`*` between arrays), dot product (`@` or `np.dotproduct()`) and other operations (`+`, `np.linalg.norm()` for vector norms, etc.)\n",
    "4. Adds each document ID and score to the DataFrame.\n",
    "5. Sorts the DataFrame by descending score.\n",
    "6. Returns the first `num_results` results."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.array([1,2])*np.array([3,4])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "doclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9c8a02b5e0e3163460a763f525f76002",
     "grade": false,
     "grade_id": "hw3-search",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [hw3-search]\n",
    "# Uses the cosine similarity to create the num_results\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def search(vectors, doclist, idf, query, num_results):\n",
    "    result=pd.DataFrame({'docid':range(0,len(doclist)),'docname':doclist,'score':[0.0]*len(doclist)})\n",
    "#     print(result)\n",
    "#     print(type(vectors), type(doclist), type(idf), type(query))\n",
    "    idf_arr=np.array(idf)\n",
    "    query_idf=np.array(query)*idf_arr\n",
    "#     vectors_idf=[]\n",
    "    i=0\n",
    "    for doc_vec in vectors:\n",
    "        vector_idf=np.array(doc_vec)*idf_arr\n",
    "        sim=np.dot(vector_idf,query_idf.transpose())/(np.linalg.norm(vector_idf)*np.linalg.norm(query_idf))\n",
    "#         print(sim)\n",
    "        result.iloc[i,2]=sim\n",
    "#         print(result.iloc[i]['score'].round(5),i)\n",
    "        i=i+1\n",
    "    result=result.sort_values(by='score',ascending=False)[:num_results]\n",
    "#     print(type(result['score']),type(result['score'][0]))\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3db62abaf637a231e25964e151a2f90e",
     "grade": true,
     "grade_id": "test-search1",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>docname</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Apple Inc..txt</td>\n",
       "      <td>0.472841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Apple II series.txt</td>\n",
       "      <td>0.357818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>Apple.txt</td>\n",
       "      <td>0.288584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>Macintosh.txt</td>\n",
       "      <td>0.264923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>In the Cloud.txt</td>\n",
       "      <td>0.213965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>Cloud computing.txt</td>\n",
       "      <td>0.213965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>Fiona Apple.txt</td>\n",
       "      <td>0.213672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>61</td>\n",
       "      <td>Apples to Apples.txt</td>\n",
       "      <td>0.213044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>87</td>\n",
       "      <td>IPhone.txt</td>\n",
       "      <td>0.111595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58</td>\n",
       "      <td>IPad.txt</td>\n",
       "      <td>0.100798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    docid               docname     score\n",
       "8       8        Apple Inc..txt  0.472841\n",
       "4       4   Apple II series.txt  0.357818\n",
       "75     75             Apple.txt  0.288584\n",
       "81     81         Macintosh.txt  0.264923\n",
       "31     31      In the Cloud.txt  0.213965\n",
       "38     38   Cloud computing.txt  0.213965\n",
       "22     22       Fiona Apple.txt  0.213672\n",
       "61     61  Apples to Apples.txt  0.213044\n",
       "87     87            IPhone.txt  0.111595\n",
       "58     58              IPad.txt  0.100798"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [test-search1]\n",
    "# Create a search query and test the values\n",
    "\n",
    "result = search(vectors, doclist, idf, doc_vector(clean_article('apple computer steve jobs'), lexicon), 10)\n",
    "display(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5afbb48b2e2849779beeb1315b8fe5a7",
     "grade": false,
     "grade_id": "test-pad39",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [CIS 545 Test Cases] (2 pts)\n",
    "print(\"[CIS 545 Homework 3] Test Case - 2 points\")\n",
    "score = score + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c0541009d08e05e36d32878b4d2be9fa",
     "grade": true,
     "grade_id": "test-search2",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>docname</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>Vladimir Putin.txt</td>\n",
       "      <td>0.662309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>Public image of Vladimir Putin.txt</td>\n",
       "      <td>0.658737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>Trump.txt</td>\n",
       "      <td>0.610337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>Donald Trump.txt</td>\n",
       "      <td>0.610337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Political career of Vladimir Putin.txt</td>\n",
       "      <td>0.593516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>The Trump Organization.txt</td>\n",
       "      <td>0.591288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>Russia under Vladimir Putin.txt</td>\n",
       "      <td>0.567728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62</td>\n",
       "      <td>Family of Donald Trump.txt</td>\n",
       "      <td>0.533129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74</td>\n",
       "      <td>Ivanka Trump.txt</td>\n",
       "      <td>0.516982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>The Putin Interviews.txt</td>\n",
       "      <td>0.494010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    docid                                 docname     score\n",
       "76     76                      Vladimir Putin.txt  0.662309\n",
       "23     23      Public image of Vladimir Putin.txt  0.658737\n",
       "72     72                               Trump.txt  0.610337\n",
       "24     24                        Donald Trump.txt  0.610337\n",
       "0       0  Political career of Vladimir Putin.txt  0.593516\n",
       "40     40              The Trump Organization.txt  0.591288\n",
       "37     37         Russia under Vladimir Putin.txt  0.567728\n",
       "62     62              Family of Donald Trump.txt  0.533129\n",
       "74     74                        Ivanka Trump.txt  0.516982\n",
       "32     32                The Putin Interviews.txt  0.494010"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [test-search2]\n",
    "# Create a search query and test the values\n",
    "\n",
    "result = search(vectors, doclist, idf, doc_vector(clean_article('Trump Putin'), lexicon), 10)\n",
    "display(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0c53f0aac1f2a797fc728d72910aba64",
     "grade": false,
     "grade_id": "test-pad40",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [CIS 545 Test Cases] (2 pts)\n",
    "print(\"[CIS 545 Homework 3] Test Case - 2 points\")\n",
    "score = score + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
